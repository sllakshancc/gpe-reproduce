{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4c164d",
   "metadata": {},
   "source": [
    "### 1. Implement Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea77aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grapheme\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import re\n",
    "\n",
    "class GraphemeBPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.grapheme_to_id = {}\n",
    "        self.id_to_grapheme = {}\n",
    "        self.merges = []\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def _get_graphemes(self, text):\n",
    "        \"\"\"Extract graphemes from text using grapheme library\"\"\"\n",
    "        return list(grapheme.graphemes(text))\n",
    "    \n",
    "    def _get_word_tokens(self, text):\n",
    "        \"\"\"Split text into words and convert each word to grapheme sequence\"\"\"\n",
    "        # Simple word splitting - you might want to improve this for Sinhala\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        word_tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word.isspace():\n",
    "                word_tokens.append([word])\n",
    "            else:\n",
    "                graphemes = self._get_graphemes(word)\n",
    "                # Add end-of-word marker to distinguish word boundaries\n",
    "                graphemes.append('</w>')\n",
    "                word_tokens.append(graphemes)\n",
    "        \n",
    "        return word_tokens\n",
    "    \n",
    "    def _get_pairs(self, word_tokens):\n",
    "        \"\"\"Get all adjacent pairs of graphemes/tokens\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word in word_tokens:\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pairs[pair] += 1\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab(self, pair, word_tokens):\n",
    "        \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
    "        new_word_tokens = []\n",
    "        \n",
    "        for word in word_tokens:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n",
    "                    # Merge the pair\n",
    "                    new_word.append(word[i] + word[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            \n",
    "            new_word_tokens.append(new_word)\n",
    "        \n",
    "        return new_word_tokens\n",
    "    \n",
    "    def train(self, texts, vocab_size=1000):\n",
    "        \"\"\"Train BPE tokenizer starting with graphemes\"\"\"\n",
    "        print(\"Starting grapheme-based BPE training...\")\n",
    "        \n",
    "        # Step 1: Extract all words and convert to grapheme sequences\n",
    "        all_word_tokens = []\n",
    "        for text in texts:\n",
    "            word_tokens = self._get_word_tokens(text)\n",
    "            all_word_tokens.extend(word_tokens)\n",
    "        \n",
    "        # Step 2: Initialize vocabulary with all graphemes\n",
    "        grapheme_freq = Counter()\n",
    "        for word_tokens in all_word_tokens:\n",
    "            for word in word_tokens:\n",
    "                for token in word:\n",
    "                    grapheme_freq[token] += 1\n",
    "        \n",
    "        # Create initial vocabulary\n",
    "        self.vocab = dict(grapheme_freq)\n",
    "        \n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Sample graphemes: {list(self.vocab.keys())[:20]}\")\n",
    "        \n",
    "        # Step 3: BPE merging process\n",
    "        for i in range(vocab_size - len(self.vocab)):\n",
    "            pairs = self._get_pairs(all_word_tokens)\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Merge the pair\n",
    "            all_word_tokens = self._merge_vocab(best_pair, all_word_tokens)\n",
    "            \n",
    "            # Update vocabulary\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab[merged_token] = pairs[best_pair]\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Merge {i}: {best_pair} -> {merged_token} (freq: {pairs[best_pair]})\")\n",
    "        \n",
    "        # Create token mappings\n",
    "        self.grapheme_to_id = {token: i for i, token in enumerate(self.vocab.keys())}\n",
    "        self.id_to_grapheme = {i: token for token, i in self.grapheme_to_id.items()}\n",
    "        \n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text using trained BPE model\"\"\"\n",
    "        word_tokens = self._get_word_tokens(text)\n",
    "        \n",
    "        # Apply merges\n",
    "        for pair in self.merges:\n",
    "            word_tokens = self._merge_vocab(pair, word_tokens)\n",
    "        \n",
    "        # Convert to IDs\n",
    "        token_ids = []\n",
    "        for word in word_tokens:\n",
    "            for token in word:\n",
    "                if token in self.grapheme_to_id:\n",
    "                    token_ids.append(self.grapheme_to_id[token])\n",
    "                else:\n",
    "                    # Handle unknown tokens - you might want to use UNK token\n",
    "                    pass\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.id_to_grapheme:\n",
    "                tokens.append(self.id_to_grapheme[token_id])\n",
    "        \n",
    "        # Join tokens and remove end-of-word markers\n",
    "        text = ''.join(tokens).replace('</w>', ' ')\n",
    "        return text.strip()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'merges': self.merges,\n",
    "            'grapheme_to_id': self.grapheme_to_id,\n",
    "            'id_to_grapheme': self.id_to_grapheme\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load trained model\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.merges = [tuple(merge) for merge in model_data['merges']]\n",
    "        self.grapheme_to_id = model_data['grapheme_to_id']\n",
    "        self.id_to_grapheme = {int(k): v for k, v in model_data['id_to_grapheme'].items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeda764",
   "metadata": {},
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e1da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\4th year\\1st sem\\Research Project\\experiments\\gpe-reproduce\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d973f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'idx': 0, 'src': 'Some 14 months later, the second calf is born.', 'tgt': 'சுமார் 14 மாதங்கள் கழித்து, இரண்டாம் கன்றை ஈனுகிறது.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = load_dataset(\"ai4bharat/samanantar\", \"ta\", split=\"train\", streaming=True)\n",
    "\n",
    "subset = list(islice(ds, 10000))\n",
    "subset_ds = Dataset.from_list(subset)\n",
    "\n",
    "print(subset_ds)\n",
    "print(subset_ds[0])\n",
    "\n",
    "# Extract Tamil texts (target side)\n",
    "tamil_texts = [item['tgt'] for item in subset_ds]\n",
    "\n",
    "# Split into train (8K) and eval (2K) sets\n",
    "train_texts_ta = tamil_texts[:8000]\n",
    "eval_texts_ta = tamil_texts[8000:10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddeca3",
   "metadata": {},
   "source": [
    "### 3. Implement calculating compression ratio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19d1b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression_ratio(tokenizer, texts):\n",
    "    \"\"\"Calculate compression ratio for a list of texts\"\"\"\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        # Count characters (including spaces)\n",
    "        char_count = len(text)\n",
    "        \n",
    "        # Count tokens\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        token_count = len(token_ids)\n",
    "        \n",
    "        total_chars += char_count\n",
    "        total_tokens += token_count\n",
    "    \n",
    "    # Compression ratio = original_size / compressed_size\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_characters': total_chars,\n",
    "        'total_tokens': total_tokens,\n",
    "        'compression_ratio': compression_ratio\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd76f4",
   "metadata": {},
   "source": [
    "### 4. function for train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "988289e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_grapheme_bpe(train_texts, lang, vocab_size=1000):\n",
    "    \"\"\"\n",
    "    Train GraphemeBPE tokenizer from your dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: Your dataset with structure {'idx': int, 'src': str, 'tgt': str}\n",
    "        vocab_size: Desired vocabulary size\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training on {len(train_texts)} {lang} texts\")\n",
    "    \n",
    "    # Show sample texts and their graphemes\n",
    "    print(f\"\\nSample training text: {train_texts[0]}\")\n",
    "    sample_graphemes = list(grapheme.graphemes(train_texts[0]))\n",
    "    print(f\"Graphemes: {sample_graphemes}\")\n",
    "    print(f\"Number of graphemes: {len(sample_graphemes)}\")\n",
    "    \n",
    "    # Initialize and train tokenizer\n",
    "    tokenizer = GraphemeBPETokenizer()\n",
    "    print(f\"\\nTraining tokenizer with vocab_size={vocab_size}...\")\n",
    "    tokenizer.train(train_texts, vocab_size=vocab_size)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"{lang}_grapheme_bpe.json\"\n",
    "    tokenizer.save(model_path)\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "    # Show vocabulary statistics\n",
    "    print(f\"\\nVocabulary Statistics:\")\n",
    "    print(f\"Total vocabulary size: {len(tokenizer.vocab)}\")\n",
    "    print(f\"Number of merges performed: {len(tokenizer.merges)}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88775ebd",
   "metadata": {},
   "source": [
    "### 5. function for train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28839aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hf_bpe_tokenizer(train_texts, lang, vocab_size=1000):\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import BPE\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "\n",
    "    tokenizer.train_from_iterator(train_texts, trainer)\n",
    "\n",
    "    tokenizer.save(f\"{lang}_hf_bpe.json\")\n",
    "    print(\"\\nHuggingFace BPE tokenizer saved.\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb845ea1",
   "metadata": {},
   "source": [
    "### 6. Train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e99dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 8000 tamil texts\n",
      "\n",
      "Sample training text: சுமார் 14 மாதங்கள் கழித்து, இரண்டாம் கன்றை ஈனுகிறது.\n",
      "Graphemes: ['சு', 'மா', 'ர்', ' ', '1', '4', ' ', 'மா', 'த', 'ங்', 'க', 'ள்', ' ', 'க', 'ழி', 'த்', 'து', ',', ' ', 'இ', 'ர', 'ண்', 'டா', 'ம்', ' ', 'க', 'ன்', 'றை', ' ', 'ஈ', 'னு', 'கி', 'ற', 'து', '.']\n",
      "Number of graphemes: 35\n",
      "\n",
      "Training tokenizer with vocab_size=1000...\n",
      "Starting grapheme-based BPE training...\n",
      "Initial vocabulary size: 188\n",
      "Sample graphemes: ['ச', 'ு', 'ம', 'ா', 'ர', '்', '<', '/', 'w', '>', ' ', '1', '4', 'த', 'ங', 'க', 'ள', 'ழ', 'ி', ',']\n",
      "Merge 0: ('ம்', '</w>') -> ம்</w> (freq: 6861)\n",
      "Merge 100: ('0', '</w>') -> 0</w> (freq: 368)\n",
      "Merge 200: ('செய்', 'ய') -> செய்ய (freq: 209)\n",
      "Merge 300: ('வி', 'ரு') -> விரு (freq: 140)\n",
      "Merge 400: ('யெ', 'கோவா') -> யெகோவா (freq: 111)\n",
      "Merge 500: ('க்', 'கூ') -> க்கூ (freq: 89)\n",
      "Merge 600: ('ப்பை', '</w>') -> ப்பை</w> (freq: 75)\n",
      "Merge 700: ('ளா', 'ல்</w>') -> ளால்</w> (freq: 62)\n",
      "Merge 800: ('வ', 'ங்கி') -> வங்கி (freq: 55)\n",
      "Final vocabulary size: 1000\n",
      "Training completed!\n",
      "\n",
      "Model saved to: tamil_grapheme_bpe.json\n",
      "\n",
      "Vocabulary Statistics:\n",
      "Total vocabulary size: 1000\n",
      "Number of merges performed: 812\n",
      "\n",
      "🔹 GPE Compression Ratio: 2.924777121008174\n"
     ]
    }
   ],
   "source": [
    "# Train GPE tokenizer\n",
    "gpe_tokenizer_ta = train_grapheme_bpe(train_texts_ta, \"tamil\", vocab_size=1000)\n",
    "\n",
    "# Evaluate GPE tokenizer\n",
    "gpe_stats_ta = calculate_compression_ratio(gpe_tokenizer_ta, eval_texts_ta)\n",
    "print(\"\\n🔹 GPE Compression Ratio:\", gpe_stats_ta['compression_ratio'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3d93e",
   "metadata": {},
   "source": [
    "### 7. Train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a5ae228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace BPE tokenizer saved.\n",
      "🔹 HF BPE Compression Ratio: 2.9452891717743204\n"
     ]
    }
   ],
   "source": [
    "# Train HF BPE tokenizer\n",
    "hf_tokenizer_ta = train_hf_bpe_tokenizer(train_texts_ta, \"tamil\", vocab_size=1000)\n",
    "\n",
    "# Evaluate HF tokenizer\n",
    "hf_stats_ta = calculate_compression_ratio(hf_tokenizer_ta, eval_texts_ta)\n",
    "print(\"🔹 HF BPE Compression Ratio:\", hf_stats_ta['compression_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250a03d",
   "metadata": {},
   "source": [
    "### 8. Overview of comparission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4849e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Compression Ratio Comparison\n",
      "GPE Tokenizer:      2.92\n",
      "HuggingFace BPE:    2.95\n"
     ]
    }
   ],
   "source": [
    "# ---- Final Comparison ----\n",
    "print(\"\\n📊 Compression Ratio Comparison\")\n",
    "print(f\"GPE Tokenizer:      {gpe_stats_ta['compression_ratio']:.2f}\")\n",
    "print(f\"HuggingFace BPE:    {hf_stats_ta['compression_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e2f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sinhala Flores+ dataset\n",
    "sin_ds = load_dataset(\"openlanguagedata/flores_plus\", \"sin_Sinh\")\n",
    "train_texts_si = [item['text'] for item in sin_ds['dev']]\n",
    "eval_texts_si = [item['text'] for item in sin_ds['devtest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06becad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 997 sinhala texts\n",
      "\n",
      "Sample training text: සඳුදා දින, ස්ටැන්ෆර්ඩ් සරසවි වෛද්‍ය පාසලේ විද්‍යාඥයෝ, සෛලයක ආකාරය අනුව එය වර්ග කළ හැකි නව දෝෂ නිර්ණය කිරීමේ මෙවලමක්: එනම්, එකක් ඇමරිකානු ඩොලර් ශතයක පමණ මුදලකින් නිෂ්පාදනය කළ හැකි සාමාන්‍ය ඉන්ක්ජෙට් මුද්‍රණ යන්ත්‍රයකින් මුද්‍රණය කළ හැකි ඉතා කුඩා චිපයක් සොයාගත් බවට නිවේදනය කළහ.\n",
      "Graphemes: ['ස', 'ඳු', 'දා', ' ', 'දි', 'න', ',', ' ', 'ස්', 'ටැ', 'න්', 'ෆ', 'ර්', 'ඩ්', ' ', 'ස', 'ර', 'ස', 'වි', ' ', 'වෛ', 'ද්\\u200d', 'ය', ' ', 'පා', 'ස', 'ලේ', ' ', 'වි', 'ද්\\u200d', 'යා', 'ඥ', 'යෝ', ',', ' ', 'සෛ', 'ල', 'ය', 'ක', ' ', 'ආ', 'කා', 'ර', 'ය', ' ', 'අ', 'නු', 'ව', ' ', 'එ', 'ය', ' ', 'ව', 'ර්', 'ග', ' ', 'ක', 'ළ', ' ', 'හැ', 'කි', ' ', 'න', 'ව', ' ', 'දෝ', 'ෂ', ' ', 'නි', 'ර්', 'ණ', 'ය', ' ', 'කි', 'රී', 'මේ', ' ', 'මෙ', 'ව', 'ල', 'ම', 'ක්', ':', ' ', 'එ', 'න', 'ම්', ',', ' ', 'එ', 'ක', 'ක්', ' ', 'ඇ', 'ම', 'රි', 'කා', 'නු', ' ', 'ඩො', 'ල', 'ර්', ' ', 'ශ', 'ත', 'ය', 'ක', ' ', 'ප', 'ම', 'ණ', ' ', 'මු', 'ද', 'ල', 'කි', 'න්', ' ', 'නි', 'ෂ්', 'පා', 'ද', 'න', 'ය', ' ', 'ක', 'ළ', ' ', 'හැ', 'කි', ' ', 'සා', 'මා', 'න්\\u200d', 'ය', ' ', 'ඉ', 'න්', 'ක්', 'ජෙ', 'ට්', ' ', 'මු', 'ද්\\u200d', 'ර', 'ණ', ' ', 'ය', 'න්', 'ත්\\u200d', 'ර', 'ය', 'කි', 'න්', ' ', 'මු', 'ද්\\u200d', 'ර', 'ණ', 'ය', ' ', 'ක', 'ළ', ' ', 'හැ', 'කි', ' ', 'ඉ', 'තා', ' ', 'කු', 'ඩා', ' ', 'චි', 'ප', 'ය', 'ක්', ' ', 'සො', 'යා', 'ග', 'ත්', ' ', 'බ', 'ව', 'ට', ' ', 'නි', 'වේ', 'ද', 'න', 'ය', ' ', 'ක', 'ළ', 'හ', '.']\n",
      "Number of graphemes: 197\n",
      "\n",
      "Training tokenizer with vocab_size=1000...\n",
      "Starting grapheme-based BPE training...\n",
      "Initial vocabulary size: 158\n",
      "Sample graphemes: ['ස', 'ඳ', 'ු', 'ද', 'ා', '<', '/', 'w', '>', ' ', 'ි', 'න', ',', '්', 'ට', 'ැ', 'ෆ', 'ර', 'ඩ', 'ව']\n",
      "Merge 0: ('ය', '</w>') -> ය</w> (freq: 1469)\n",
      "Merge 100: ('ප', 'ව') -> පව (freq: 82)\n",
      "Merge 200: ('ග', 'ත්</w>') -> ගත්</w> (freq: 47)\n",
      "Merge 300: ('එ', 'ම</w>') -> එම</w> (freq: 34)\n",
      "Merge 400: ('අ', 'ඩු</w>') -> අඩු</w> (freq: 24)\n",
      "Merge 500: ('ශ', 'ක්') -> ශක් (freq: 20)\n",
      "Merge 600: ('කා', 'ර්') -> කාර් (freq: 17)\n",
      "Merge 700: ('කර', 'න්නේ</w>') -> කරන්නේ</w> (freq: 14)\n",
      "Merge 800: ('අ', 'ර්') -> අර් (freq: 12)\n",
      "Final vocabulary size: 1000\n",
      "Training completed!\n",
      "\n",
      "Model saved to: sinhala_grapheme_bpe.json\n",
      "\n",
      "Vocabulary Statistics:\n",
      "Total vocabulary size: 1000\n",
      "Number of merges performed: 842\n",
      "\n",
      "🔹 GPE Compression Ratio: 2.4359331735147087\n"
     ]
    }
   ],
   "source": [
    "# Train GPE tokenizer\n",
    "gpe_tokenizer_si = train_grapheme_bpe(train_texts_si, \"sinhala\", vocab_size=1000)\n",
    "\n",
    "# Evaluate GPE tokenizer\n",
    "gpe_stats_si = calculate_compression_ratio(gpe_tokenizer_si, eval_texts_si)\n",
    "print(\"\\n🔹 GPE Compression Ratio:\", gpe_stats_si['compression_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a565b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace BPE tokenizer saved.\n",
      "🔹 HF BPE Compression Ratio: 2.6817242578612492\n"
     ]
    }
   ],
   "source": [
    "# Train HF BPE tokenizer\n",
    "hf_tokenizer_si = train_hf_bpe_tokenizer(train_texts_si, \"sinhala\", vocab_size=1000)\n",
    "\n",
    "# Evaluate HF tokenizer\n",
    "hf_stats_si = calculate_compression_ratio(hf_tokenizer_si, eval_texts_si)\n",
    "print(\"🔹 HF BPE Compression Ratio:\", hf_stats_si['compression_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b6199bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Compression Ratio Comparison\n",
      "GPE Tokenizer:      2.44\n",
      "HuggingFace BPE:    2.68\n"
     ]
    }
   ],
   "source": [
    "# ---- Final Comparison ----\n",
    "print(\"\\n📊 Compression Ratio Comparison\")\n",
    "print(f\"GPE Tokenizer:      {gpe_stats_si['compression_ratio']:.2f}\")\n",
    "print(f\"HuggingFace BPE:    {hf_stats_si['compression_ratio']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
